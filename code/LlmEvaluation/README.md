# üéôÔ∏è LLM Emotion & Strategy Benchmark

This project is a testing framework designed to evaluate the ability of various LLM models to predict the correct **emotion** and the best **response strategy** for voice interactions. Inferences are based on the audio's prosodic values (such as *valence* and *arousal*), the emotion predicted from the audio file, and the text transcription.

The system automates API calls to multiple providers, calculates operational costs, and evaluates responses using an **LLM-as-a-Judge** approach, determining the best-performing model based on rigorous metrics.

---

## üìÅ Repository Structure

The codebase is organized into modules to separate API calling logic, prompts, and results:

* **`/provider/`**: Contains the scripts to make API requests to the supported providers (**Anthropic, Gemini, Groq, HuggingFace, OpenAI**).
* **`/prompt/`**: Gathers all text files containing the system prompts (both for the models being tested and for the Judge model).
* **`/file/test_case/test_case.json`**: The input dataset containing the test cases to analyze.
* **`/file/models_price`** (Excel): The list of models to test, including the cost per input and output token to dynamically calculate the expense of each run.
* **`/file/output/`**: Destination folder for the final results. It will host the inference JSON files (`final_benchmark_result`) and the Excel reports with the rankings and Judge scores.

---

## ‚öôÔ∏è Installation & Setup

To run the code, you need to set up your local environment and configure your API keys.

1. **Create and activate a virtual environment**:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

```


2. **Install the requirements**:
```bash
pip install -r requirements.txt

```


3. **Configure environment variables**:
Copy the example configuration file and insert the necessary tokens for the various providers.
```bash
cp env-example .env

```



---

## üöÄ Usage

The benchmark process runs in two sequential phases.

### 1. Evaluation Phase (Inference)

The first step queries all models defined in the Excel file in parallel to get predictions on emotion and strategy for each test case.

```bash
python run_evaluation.py

```

* **What it does**: Takes the tests in `test_case.json`, makes requests to the models via parallel threads, and saves the raw results and operational costs.
* **Optional arguments**:
* `--debug`: Runs the process only on the first test case (useful for testing the flow).
* `--max-workers <int>`: Modifies the number of simultaneous threads (default: 5).


* **Output**: Creates the `benchmark_report.xlsx` file and the `final_benchmark_result.json` backup in the output folder.

### 2. Judging Phase (LLM-as-a-Judge)

The second step analyzes the results generated by the previous step, using a high-performance LLM to evaluate the quality of each response (claude-opus-4.6).

```bash
python run_judge.py

```

* **What it does**: Takes the previously generated JSON file. The Judge model assigns detailed scores to key parameters (Emotion Match, Strategy Match, Relevance, Empathic Response, etc.) and calculates a total score (out of 100).
* **Optional arguments**:
* `--debug`: Evaluates only the first case.
* `--max-workers <int>`: Modifies the judge's threads (default: 1, recommended to prevent severe rate-limits during evaluations).


* **Output**: Generates a complete and formatted Excel report containing the final ranking to declare the best LLM and detailed pivot tables. Includes progressive saves in case of interruptions.